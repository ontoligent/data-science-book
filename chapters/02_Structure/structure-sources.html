<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.262">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>structure-sources</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="structure-sources_files/libs/clipboard/clipboard.min.js"></script>
<script src="structure-sources_files/libs/quarto-html/quarto.js"></script>
<script src="structure-sources_files/libs/quarto-html/popper.min.js"></script>
<script src="structure-sources_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="structure-sources_files/libs/quarto-html/anchor.min.js"></script>
<link href="structure-sources_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="structure-sources_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="structure-sources_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="structure-sources_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="structure-sources_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">



<section id="primary-sources" class="level1">
<h1>Primary Sources</h1>
<section id="about-the-sources" class="level2">
<h2 class="anchored" data-anchor-id="about-the-sources">About the Sources</h2>
<p>The primary sources on which the conclusions of this essay are based comprise a variety of documents, from technical journals to blog posts to internal reports. They come from a range of viewpoints, from data analysis and statistics to data mining and data science <em>per se</em>. For the purposes of the essay, we select a more or less representative subset across these axes of variation. With respect to representativeness, in some cases a document was chosen for its influence, in others, such as the post by Dataman, because it is considered more or less typical of a common genre.</p>
<p>The documents chosen are listed below in chronological order, beginning with Tukey’s seminal essay on data analysis and ending with contempory explainers. Included also are the definitions of the CRISP-DM and KDD processes which are the most developed pipeline models.</p>
<p>Each source entry below contains a short description of the source and its context, and then a list of the phases cited by the authors as fundamental to data processing. These phases are also mapped onto the standard sequence described in the main part of this essay, listed here for convenience.</p>
<ol type="1">
<li>Understand</li>
<li>Plan</li>
<li>Collect</li>
<li>Store</li>
<li>Clean</li>
<li>Explore</li>
<li>Prepare</li>
<li>Model</li>
<li>Interpret</li>
<li>Communicate</li>
<li>Deploy</li>
<li>Reflect</li>
</ol>
<p>Mappings are indicated by an arrow pointing to the subset of terms from the standard sequence, e.g.&nbsp;… <span class="math inline">\(\rightarrow [Explore]\)</span> These mappings are also aggregated into a composite pipeline and displayed the table below; each model row is referenced by its key as defined in the entries.</p>
<p>Note that in most cases these phases are explicitly described as a process and often as a pipeline. When they are not, the implication is strong. In some cases, the process is likened to a cycle, emphasizing the connection between the endpoints of the pipeline, which is also emphasized by the 4+1 model.</p>
<p>A final feature added to each entry is a two-value indicator of bias — statistics and data mining. This is meant to capture the intellectual origin of the model, given that statistics and data mining define the poles of one of main axes of variance that defines the field of data science. This difference roughly corresponds to the “two cultures” described by Breiman <span class="citation" data-cites="breimanStatisticalModelingTwo2001">@breimanStatisticalModelingTwo2001</span>.</p>
</section>
<section id="list-of-sources" class="level2">
<h2 class="anchored" data-anchor-id="list-of-sources">List of Sources</h2>
<section id="tukey-on-data-analysis" class="level3">
<h3 class="anchored" data-anchor-id="tukey-on-data-analysis">Tukey on Data Analysis</h3>
<p>Key: <code>Tukey</code><br>
Year: 1962<br>
Source: <span class="citation" data-cites="tukeyFutureDataAnalysis1962">@tukeyFutureDataAnalysis1962</span> <a href="https://www.jstor.org/stable/2237638#metadata_info_tab_contents">URL</a><br>
Bias: Statistics</p>
<p>In this classic essay, Tukey introduces the concept of data analysis, which he distinguishes from mathematical statistics and likens to an empirical science. He defines data analysis as an empirical process with phases including “… procedures for <strong>analyzing</strong> data, techniques for <strong>interpreting</strong> the results of such procedures, ways of <strong>planning</strong> the <strong>gathering</strong> of data to make its analysis easier, more precise or more accurate, and all the machinery and results of (mathematical) statistics which apply to analyzing data” (p.&nbsp;2). Unpacking this statement yields a four phase model.</p>
<ol type="1">
<li><strong>Planning</strong>: This phase includes “ways of planning the the gather of data to make its analysis easier.” <span class="math inline">\(\rightarrow [Plan]\)</span></li>
<li><strong>Gathering</strong>: The gathering of data, either through creation or by acquisition of “data already obtained” (p.&nbsp;40). Includes also the shaping of data “to make its analysis easier,” which corresponds to our concept of Preparation. <span class="math inline">\(\rightarrow [Collect, Prepare]\)</span></li>
<li><strong>Analyzing</strong>: This is where data are analyzed with “all the machinery and results of (mathematical) statistics.” <span class="math inline">\(\rightarrow [Explore, Model]\)</span></li>
<li><strong>Interpreting</strong>: “techniques for interpreting the results of” analysis. <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
</ol>
</section>
<section id="fayyad-on-kdd" class="level3">
<h3 class="anchored" data-anchor-id="fayyad-on-kdd">Fayyad on KDD</h3>
<p>Key: <code>KDD</code><br>
Year: 1996<br>
Source: <span class="citation" data-cites="fayyadKnowledgeDiscoveryData1996">@fayyadKnowledgeDiscoveryData1996</span> <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-014.pdf?utm_campaign=ml4devs-newsletter&amp;utm_medium=email&amp;utm_source=Revue%20newsletter">URL→</a><br>
Bias: Data Mining<br>
</p>
<p>KDD, or Knowledge Discovery in Databases, emerged in the late 1980s as both datasets and the computational resources to work with them became abundant. These resources included commercial databases and personal computers. In many ways the most adjacent field to contemoporary data science, this approach is unabashedly dedicated to finding patterns in data prior to developing a probabilistic model to justify their use. Fayyad’s essay identifies five steps <span class="citation" data-cites="fayyadKnowledgeDiscoveryData1996">[@fayyadKnowledgeDiscoveryData1996: 84]</span>. He emphasizes the highly iterative and cyclical nature of the process, arguing that it “may contain loops between any two steps.” Another significant aspect of this conception of the pipeline is the role of exploration in the analytical phase: “Data Mining is a step in the KDD process consisting of applying data analysis and discovery algorithms that, under acceptable computational efficiency limitations, produce a particular enumeration of patterns over the data ….” (p.&nbsp;83)</p>
<ol type="1">
<li><strong>Selection</strong>: Creating a target data set, or focusing on a subset of variables or data samples, on which discovery is to be performed. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Pre-processing</strong>: Cleaning and pre processing the data in order to obtain consistent data. <span class="math inline">\(\rightarrow [Clean]\)</span></li>
<li><strong>Transformation</strong>: Transformation of the data using dimensionality reduction and other methods. <span class="math inline">\(\rightarrow [Prepare]\)</span></li>
<li><strong>Data Mining</strong>: Searching for patterns of interest in a particular representational form, depending on the DM objective (usually, prediction). <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Interpretation/Evaluation</strong>: Interpretation and evaluation of the mined patterns. <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
</ol>
</section>
<section id="azevedo-on-semma" class="level3">
<h3 class="anchored" data-anchor-id="azevedo-on-semma">Azevedo on SEMMA</h3>
<p>Key: <code>SEMMA</code><br>
Year: 1996<br>
Source: <span class="citation" data-cites="azevedoKDDSEMMACRISPDM2008">@azevedoKDDSEMMACRISPDM2008</span><br>
Bias: Statistics</p>
<p>The SEMMA model was developed the by SAS institute in 1996 as part of the documentation for their product, SAS Enterprise Miner. Even so, the model is referenced outside of this context, often as a comparison to KDD and CRISP-DM. Its bias towards statististics is evident in the first step.</p>
<ol type="1">
<li><strong>Sample</strong>: Sampling the data by extracting a portion of a large data set big enough to contain the significant information, yet small enough to manipulate quickly. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Explore</strong>: Exploration of the data by searching for unanticipated trends and anomalies in order to gain understanding and ideas <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Modify</strong>: Modification of the data by creating, selecting, and transforming the variables to focus the model selection process <span class="math inline">\(\rightarrow [Prepare]\)</span></li>
<li><strong>Model</strong>: Modeling the data by allowing the software to search automatically for a combination of data that reliably predicts a desired outcome. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Assess</strong>: Assessing the data by evaluating the usefulness and reliability of the findings from the DM process and estimate how well it performs. <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
</ol>
</section>
<section id="hayashi-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="hayashi-on-data-science">Hayashi on Data Science</h3>
<p>Key: <code>Hayashi</code><br>
Year: 1998<br>
Source: <span class="citation" data-cites="hayashiDataScienceClassification1998">@hayashiDataScienceClassification1998</span> <a href="https://link.springer.com/chapter/10.1007/978-4-431-65950-1_3">URL→</a><br>
Bias: Statistics</p>
<p>The Japanese statistician Chikio Hayashi adopted the term “data science” in the early 1990s to define a field that did not succumb to what he saw to be the errors of both statistics and data analysis. He argued that mathematical statistics had become too attached to problems of inference and removed from reality, while data analysis had lost interest in understanding the meaning of the data it deals with. His definition of data science is decidely processual: “Data Science consists of three phases: design for data, collection of data and analysis on data. It is important that the three phases are treated with the concept of unification based on the fundamental philosophy of science …. In these phases the methods which are fitted for the object and are valid, must be studied with a good perspective.” (p.&nbsp;41) Similar to KDD and CRISM-PM, Hayashi envisioned this process as a spiral, oscillating between poles if what he called “diversification” and “simplification.” Note also that each of these terms, as described, comprises more than on of the standard sequence phases.</p>
<ol type="1">
<li><strong>Design</strong>: Surveys and experiments are developed to capture data from “multifarious phenomena.” <span class="math inline">\(\rightarrow [Understand, Plan]\)</span></li>
<li><strong>Collection</strong>: Phenomena are expressed as multidimensional or time-series data; properties of the data are made clear. At this stage, data are too complicated to draw clear conclusions. (Representation) <span class="math inline">\(\rightarrow [Collect, Explore, Prepare]\)</span></li>
<li><strong>Analysis</strong>: By methods of classification, multidimensional data analysis, and statistics, data structure is revealed. Simplification and conceptualization. Also yields understanding of deviations of the model, which begins the cycle anew. (Revelation) <span class="math inline">\(\rightarrow [Model, Interpet]\)</span></li>
</ol>
</section>
<section id="wirth-and-hipp-on-crisp-dm" class="level3">
<h3 class="anchored" data-anchor-id="wirth-and-hipp-on-crisp-dm">Wirth and Hipp on CRISP-DM</h3>
<p>Key: <code>CRISPDM</code><br>
Year: 1999<br>
Source: <span class="citation" data-cites="wirthCRISPDMStandardProcess1999">@wirthCRISPDMStandardProcess1999</span> <a href="http://www.cs.unibo.it/~danilo.montesi/CBD/Beatriz/10.1.1.198.5133.pdf">URL→</a><br>
Bias: Data Mining</p>
<p>By the late 1990s, the practice of data mining had become widespread in industry and globally. In 1999 the Cross Industry Standard Process for Data Mining (CRISP-DM) was developed in Europe as a comprehensive and general model to support the use of data mining in a broad range of sectors in a principled manner. Designed to work within a project management framework, this model is by far the most developed, and it continues to influence the field of data science to this day. Like KDD before it, the model emphasizes the cyclic and recursive nature of the process, and this perspective is reflected in the circular diagram that often accompanies its presentation. The steps below are based on the summary presented in Wirth and Hipp’s essay.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/1024px-CRISP-DM_Process_Diagram.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Process diagram showing the relationship between the different phases of CRISP-DM (Wikipedia)</figcaption><p></p>
</figure>
</div>
<ol type="1">
<li><strong>Business Understanding</strong>: Understanding project objectives and requirements from a business perspective. Includes the development of a plan. <span class="math inline">\(\rightarrow [Understand, Plan]\)</span></li>
<li><strong>Data Understanding</strong>: The initial data collection and activities to get familiar with the data, e.g.&nbsp;to identify data quality problems, to discover first insights into the data, or to detect interesting subsets to form hypotheses for hidden information. This is really two phases — <strong>Collection</strong> and <strong>Exploration</strong> — which are combined because of their close, iterative relationship. <span class="math inline">\(\rightarrow [Collect, Explore]\)</span></li>
<li><strong>Data Preparation</strong>: Construction of the final dataset for analytical use. Tasks include table, record, an attribute selection, data cleaning, construction of new attributes, and transformation of data for modeling tools. <span class="math inline">\(\rightarrow [Clean, Prepare]\)</span></li>
<li><strong>Modeling</strong>: Modeling techniques are selected and applied, parameters calibrated. Modeling techniques include a broad range of unsupervised and supervised methods. As with KDD, there is an emphasis on pattern discovery, which has the effect of promoted methods that other models place squarely in the Explore phase of the standard sequence. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Evaluation</strong>: Evaluation of model performance by both intrinsic and extrinsic measures. Regarding the latter, a key objective is to determine if an important business issue has not been sufficiently considered. <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
<li><strong>Deployment</strong>: The knowledge gained by the model is presented in a way that the customer can use it. This may be something as simple as a report or as complex as a repeatable data mining process. In many cases the user, not the data analyst, will carry out the deployment. <span class="math inline">\(\rightarrow [Deploy]\)</span></li>
</ol>
</section>
<section id="mason-and-wiggins-on-osemi" class="level3">
<h3 class="anchored" data-anchor-id="mason-and-wiggins-on-osemi">Mason and Wiggins on OSEMI</h3>
<p>Key: <code>OSEMI</code><br>
Year: 2010<br>
Source: <span class="citation" data-cites="masonTaxonomyDataScience2010">@masonTaxonomyDataScience2010</span> <a href="https://sites.google.com/a/isim.net.in/datascience_isim/taxonomy">URL→</a><br>
Bias: Data Mining</p>
<p>After the phrase “data science” went viral (circa 2009), there were many efforts to make sense of the idea. In 2010 Drew Conway posted his Venn diagram of data science <span class="citation" data-cites="conwayDataScienceVenn2010">[@conwayDataScienceVenn2010]</span>. The same year, another influential model, based explicitly on the pipeline, came from Mason and Wiggins in a blog post hosted at O’Reilly’s Tech Radar site. In contrast to previous models rooted in statistics, this model assumes that data are abundant and available, such as data scrapable from the Web.</p>
<ol type="1">
<li><strong>Obtain</strong>: Gather data from relevant sources through APIs, web scraping, etc. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Scrub</strong>: Clean data and convert data to machine readable formats. Clearning includes handling missing data, inconsistent labels, or awkward formatting; stripping extraneous characters; normalizing values, etc. <span class="math inline">\(\rightarrow [Clean, Prepare]\)</span></li>
<li><strong>Explore</strong>: Find significant patterns and trends using statistical and data analytic methods, such as visualizing, clustering. Also includes transformations of the for more effective analysis, such as dimensionality reduction. <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Model</strong>: Construct methods to predict and forecast. These methods include those of inferential statistics and predictive machine learning. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Interpret</strong>: Making sense of the results as well as evaluating the performance of models. May involve domain experts. Also includes methods such as regularization that make models interpretable to those who use them, e.g.&nbsp;scientists or business people. <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
</ol>
</section>
<section id="ojeda-et-al.-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="ojeda-et-al.-on-data-science">Ojeda, et al.&nbsp;on Data Science</h3>
<p>Key: <code>Ojeda+</code><br>
Year: 2014<br>
Source: <span class="citation" data-cites="ojedaPracticalDataScience2014">@ojedaPracticalDataScience2014</span> <a href="https://www.packtpub.com/product/practical-data-science-cookbook-second-edition/9781787129627">URL→</a><br>
Bias: Data Mining</p>
<p>By 2014, data science had become a widespread practice in industry and the academic, and explanations of its nature became the subject of many books. This text is one of a genre that presents the field as a process, perhaps due to the influence of the CRISP-DM and OSEMI models, and uses the expression pipeline throughout. Note that the model defined in this book is not presented here as canonical. It suffers from various inconsistences, such as the labeling of steps in the text representation of the pipeline versus those on diagrams. It is included to demonstrate the pervasiveness of the model.</p>
<ol type="1">
<li><strong>Acquisition</strong>: Acquire the data from relational databases, NoSQL and document stores, web scraping, distributed databases (e.g.&nbsp;HDFS on a Hadoop platform), RESTful APIs, flat files, etc. Consistent with the other data mining models, the emphasis here is on working with available data, not generating it. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Exploration and understanding</strong>: Understand the data and how it was collected or produced; this often requires significant exploration. Note that this step does <em>not</em> correspond to exploration in the sense of exploratory data analysis (EDA). Rather, it reflects the position of the data scientist as the receiver of someone else’s data and the need to infer what would normally belong to the first step of the standard squence <span class="math inline">\(Understand\)</span>. <span class="math inline">\(\rightarrow [Understand]\)</span></li>
<li><strong>Munging, wrangling, and manipulation</strong>: Convert the data into the form required for analysis. This includes a wide range of activities, such as those mentioned in previous models. However, it also conflates the standard phases <span class="math inline">\(Clean\)</span> and <span class="math inline">\(Prepare\)</span>. <span class="math inline">\(\rightarrow [Clean, Prepare]\)</span></li>
<li><strong>Analysis and modeling</strong>: Apply statistical and machine learning methods, including clustering, categorization, and classification. One presumes that the standard step of <span class="math inline">\(Explore\)</span> is included here. <span class="math inline">\(\rightarrow [Explore, Model]\)</span></li>
<li><strong>Communicating and operationalizing</strong>: At the end of the pipeline, we need to give the data back in a compelling form and structure, sometimes to ourselves to inform the next iteration, and sometimes to a completely different audience. The data products produced can be a simple one-off report or a scalable web product that will be used interactively by millions. <span class="math inline">\(\rightarrow [Communicate, Deploy]\)</span></li>
</ol>
</section>
<section id="caffo-et-al.-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="caffo-et-al.-on-data-science">Caffo, et al.&nbsp;on Data Science</h3>
<p>Key: <code>Caffo+</code><br>
Year: 2015<br>
Source: <span class="citation" data-cites="caffoExecutiveDataScience2015">@caffoExecutiveDataScience2015</span> <a href="https://leanpub.com/eds">URL→</a><br>
Bias: Statistics</p>
<p>By 2015, many universities had begun offering degrees in data science, typically at the masters’ level, with the intention of meeting the high demand for data scientists. Professors Caffo, Peng, and Leek’s book was written to accompany a course in Exectutive Data Science, offered by Johns Hopkins University through <em>Coursera</em>. Their model is relatively high level, consisting of five phases, given the target audience of those in charge of data science teams. As with other models, this model emphasizes the iterative nature of each phase, both internally and between phases. And as with many statistics-oriented conceptions of data science, this model emphasizes the Understand phase and skips over the technical issues of storing and modeling the data.</p>
<ol type="1">
<li><strong>Question</strong>.: Pose a research question and specify what is to be learned from data to answer it. The question determines the data to be obtained and the type of analysis to perform. Included determing the type of question, including descriptive, exploratory, inferential, causal, predictive, and mechanistic. An alternate approach here is <em>hypothesis generation</em>, which may be suitable when data already exist but a question is not well-developed. In this scenario, the data scientist may skip to the next step to determine the value of the data. Once a question is developed, then it may be necessary to acquire more data, and then go through the process. <span class="math inline">\(\rightarrow [Question, Collect]\)</span></li>
<li><strong>Exploratory data analysis</strong>: Explore the data to determine if the data are suitable for answering the question and if more data need to be collected. For example, determine if there are enough data and if it is missing key variables. In addition, develop a sketch of the solution. Include a freamework for challenging results and to develop robust evidence for answering your question.&nbsp; <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Formal modeling</strong>: Identify the parameters to estimate based on the sketch. &nbsp; <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Interpretation</strong>: Determine if the modeling results align with the initial expections during the Question phase and before the acquisition of data. Consider the totality of the evidence developed after attempting to fit different models, weighing the different pieces of evidence. &nbsp; <span class="math inline">\(\rightarrow [Interpret]\)</span></li>
<li><strong>Communication</strong>: Communicate findings to various audiences, either internal to the organization or external. Includes translating findings into action by virtue of effectively communicating results to decision-makers. <span class="math inline">\(\rightarrow [Communicate]\)</span></li>
</ol>
</section>
<section id="donaho-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="donaho-on-data-science">Donaho on Data Science</h3>
<p>Key: <code>Donoho</code><br>
Year: 2017<br>
Source: <span class="citation" data-cites="donoho50YearsData2017">@donoho50YearsData2017</span> <a href="https://doi.org/10.1080/10618600.2017.1384734">URL→</a><br>
Bias: Statistics</p>
<p>As data science became viral in the 2010s, academic statisticians frequently expressed concern that they were “disconnected from the new (and vaguely defined) community of data scientists, who are completely identified with Big Data in the eyes of the media and policymakers” <span class="citation" data-cites="rodriguezBigDataBetter2012">[@rodriguezBigDataBetter2012]</span>. “Aren’t <em>We</em> Data Scientists?” asked Marie Davidian, then president of the American Statistical Association, in 2013 <span class="citation" data-cites="davidianArenWeData2013">[@davidianArenWeData2013]</span>. In response to this growing sentiment, Donoho’s essay reads as a manifesto for the reclaiming of data science by academic statistics. In it, he defines six divisions of Greater Data Science, each containing a set of subactivities that roughly map to the pipeline model described here.</p>
<p>It is important to note that Donoho’s model is more abstract than a pipeline description and therefore not all of the divisions and subactivities directly map onto the sequence. <strong>Data visualization and Presentation</strong> defines a general practice, although from the description it clearly maps onto two phases, Explore and Communicate. <strong>Computing with Data</strong> refers to knowledge of programming languages for data analysis and data processing as well as knowledge of how to use cluster and cloud computing resources at scale. It also includes how to develop workflows which organize work. Clearly, this area belongs to no phase in particular but instead characterizes the broader context in which the data science pipeline operates. The identification of workflows, which are the focus of the Science about Data Science division, also suggests that Donoho is working at a higher level of abstraction than the other models, which places it alongside the of the current essay. The following phases are inferred from Donoho’s descriptions.</p>
<ol type="1">
<li><strong>Gathering</strong>: This includes both experimental design, modern data gathering techniques, and identification of existing data resources, from signal data to websites. <span class="math inline">\(\rightarrow [Plan, Collect]\)</span></li>
<li><strong>Preparation</strong>: Identification of anomalies and artifacts in the data and handling them by reformatting, recoding, grouping, smoothing, subsetting, etc. <span class="math inline">\(\rightarrow [Clean]\)</span></li>
<li><strong>Exploration</strong>: Application of EDA to sanity-check data and expose unexpected features. Includes data visualization, which Donoho separates out into a separate division and combines with visualization activities involved in interpretation and communication. <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Modern Databases</strong>: Transform and restructure data as found in source files, such as CSV files and spreadsheets, and databases, into a forms more suitable for analysis. <span class="math inline">\(\rightarrow [Prepare]\)</span></li>
<li><strong>Mathematical Representations</strong>: Application of mathematical structures for to extract features from special kinds of data, including acoustic, image, sensor, and network data. For example, the application of the Fourier transform to acousting data or the wavelet transform to image data. <span class="math inline">\(\rightarrow [Prepare]\)</span></li>
<li><strong>Data Modeling</strong>: Appliction of methods from both traditional statistics and contemporary machine learning. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Presentation</strong>: The creation of sophisticated graphics, dashboards, and visualizations to present conclusions to stakeholders. <span class="math inline">\(\rightarrow [Communicate]\)</span></li>
<li><strong>Science about Data Science</strong>: In the spirit of Tukey’s “science of data analysis,” this is the evaluation of what data scientists actually do and produce. Includes the identificatin and study of commonly occurring analytical and processing workflows. <span class="math inline">\(\rightarrow [Reflect]\)</span></li>
</ol>
</section>
<section id="géron-on-machine-learning" class="level3">
<h3 class="anchored" data-anchor-id="géron-on-machine-learning">Géron on Machine Learning</h3>
<p>Key: <code>Géron</code><br>
Year: 2017<br>
Source: <span class="citation" data-cites="geronHandsOnMachineLearning2017">@geronHandsOnMachineLearning2017</span> <a href="https://www.investincotedor.fr/sites/default/files/webform/pdf-hands-on-machine-learning-with-scikit-learn-and-tensorflow-conce-aurlien-gron-pdf-download-free-book-21c7262.pdf">URL→</a><br>
Bias: Data Mining</p>
<p>Géron’s text is a classic among practicing data scientists interested in using machine learning in a business setting, covering everything from regression to deep learning from a practical, code-centric perspective. Written with “minimal theory,” the book demostrates the entrenched nature of the pipeline model, especially as it has been become a software development pattern hard-coded into both SciKit Learn and TensorFlow. This usage reflects the fact that within machine learning, “pipeline” has taken on a more specific meaning — “a sequence of data processing components” — than we are using here. These components are units software within a system, not the phases of labor associated with the work of the data scientist. Nevertheless, Géron’s text describes a labor pipeline within which the software pipeline is embedded, the steps of an “end-to-end” classification project.</p>
<ol type="1">
<li><strong>Look at the big picture</strong>: Frame the problem by defining the business objective and the specific goals of the model. This may include defining a specific performance measure, such as a loss function. Consider that the model is a a means to an end. <span class="math inline">\(\rightarrow [Understand]\)</span></li>
<li><strong>Get the data</strong>: This consists of setting up a compute workspace and downloading the data. This step also includes getting to know the data and preparing a test set. <span class="math inline">\(\rightarrow [Get, Prepare]\)</span></li>
<li><strong>Discover and visualize the data</strong> to gain insights: Go into more depth exploring the data, using EDA methods to investigate correlations and experiment with attribute combinations. <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Prepare the data</strong> for Machine Learning algorithms: This step involves transforming and structuring the data in forms suitable for the algorithms that with fit the data to a model. This includes imputing missing data, handling non-numeric data, feature scaling, etc. This step contains its own pipeline. <span class="math inline">\(\rightarrow [Clean, Prepare]\)</span></li>
<li><strong>Select a model and train it</strong>: Apply models deemed appropriate to the data and compare results. Apply evaluation methods such as cross-validation to compare model results. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Fine-tune your model</strong>: Once the list of candidate models is shortened, fine-tune their parameters by using various seach methods, e.g.&nbsp;grid, randomized, or ensemble. Also includes evaluating the models on test sets. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Present your solution</strong>: : This step includes presenting to stakeholders what was learned, what worked and what did not, what assumptions were made, and what the system’s limitations are. It also includes documenting everything, creating user-friendly presentations with clear visualizations and easy-to-remember statements. Géron refers to this as the “prelaunch phase,” presumably because the component must be approved to go on to the next phase. <span class="math inline">\(\rightarrow [Communicate]\)</span></li>
<li><strong>Launch, monitor, and maintain your system</strong>: This step includes converting your model into a production-ready component that can become a functioning piece of the overall pipeline. This may mean creating a web service. <span class="math inline">\(\rightarrow [Deploy]\)</span></li>
</ol>
</section>
<section id="das-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="das-on-data-science">Das on Data Science</h3>
<p>Key: <code>Das</code><br>
Year: 2019<br>
Source: <span class="citation" data-cites="dasDataScienceLife2019">@dasDataScienceLife2019</span> <a href="https://web.archive.org/web/20191113225625/https://towardsdatascience.com/data-science-life-cycle-101-for-dummies-like-me-e66b47ad8d8f?gi=261acdd4c903">URL→</a><br>
Bias: Data Mining</p>
<p>This essay belongs to a genre of self-publication that attempts to explain concepts in data science to the public. It is typical of platforms like <em>Medium</em> and what used to be called the blogosphere. It is included here to represent the commonplace nature of the pipeline as a rhetorical device for explaining data science. Here, the pipeline is called a “life-cycle,” although the term pipeline is used as well. The cyclical nature of the process is emphasized by including the first step as last step of the process. [Note that this essay was removed from the web by the author; a link to Internet Archive URL is included for completeness.]</p>
<ol type="1">
<li><strong>Business Understanding</strong>: Understand the problem you are trying to solve and how data can be used to support a solution or decision. Identify central objectives and variables that need to be predicted. (Here the author implies that methods such as regression and clustering are objectives.) <span class="math inline">\(\rightarrow [Understand]\)</span></li>
<li><strong>Data Mining</strong>: Gathering the data from various sources. This may invovle extracting data from a legacy database or webscraping. The author correctly notes that this step should not be lumped together with cleaning. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Data Cleaning</strong>: This step includes cleaning and preparing the data, also know as “data janitor work.” This step takes most of the data scientist’s time because there are so many reasons that data may need cleaning. Also includes handling missing data.<br>
<span class="math inline">\(\rightarrow [Clean]\)</span></li>
<li><strong>Data Exploration</strong>: This is the brainstorming phase of data analysis, where one discovers patterns and biases in the data. Involves using the basic tools of EDA but also creating interactive visualizations to allow drilling down into specific points, e.g.&nbsp;to explore the story behind outliers. Here also one begins to form hypotheses about the data to be developed. <span class="math inline">\(\rightarrow [Explore]\)</span></li>
<li><strong>Feature Engineering</strong>: This is the process of using domain knowledge to transform the data into informative features that represent the business problem. This stage directly influences the accuracy of the predictive model constructed in the next stage. Methods include feature selection (i.e.&nbsp;dimensionality reduction) and constructing new features that will aid in the modeling process. <span class="math inline">\(\rightarrow [Prepare]\)</span></li>
<li><strong>Predictive Modeling</strong>: The application of machine learning methods to the data. Includes training several models and evaluating their performance, as well as applying statistical methods and tests to ensure that the outcomes from the models make sense and are significant. Based on the questions developed in the business understanding stage, this is where a model is selected. Model selection will depend on the size, type and quality of the data, availability of computational resources, and the type of output required. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Data Visualization</strong>: This step combines expertise from the fields of communication, psychology, statistics, and art, with an ultimate goal of communicating the insightw from the model in a simple yet effective and visually pleasing way. <span class="math inline">\(\rightarrow [Communicate]\)</span></li>
</ol>
</section>
<section id="dataman-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="dataman-on-data-science">Dataman on Data Science</h3>
<p>Key: <code>Dataman</code><br>
Year: 2020<br>
Source: <span class="citation" data-cites="datamanDataScienceModeling2020">@datamanDataScienceModeling2020</span> <a href="https://towardsdatascience.com/data-science-modeling-process-fa6e8e45bf02">URL→</a><br>
Bias: Data Mining</p>
<p>Another example of a self-published explainer essay, this one describes the data science “modeling process” and aligns it with six consultative roles. The other defines eight steps to the process. Curiously, althhough this pipeline focuses on the details of training models, it does not include training the model itself as a step.</p>
<ol type="1">
<li><strong>Set the objectives</strong>: This step includes defining the goals of the model as well as its scope and risk factors. These will determine what data to collect, and whether the cost to collect the data can be justified by the impact of the model. <span class="math inline">\(\rightarrow [Understand]\)</span></li>
<li><strong>Communicate with key stakeholders</strong>: This step involves ongoing aligning expected outcomes with key stakeholders. This step is unique among the pipelines by being place so early in the process. We associate it with the <span class="math inline">\(Understand\)</span> phase because it essentially broads the group for whom understanding matters. <span class="math inline">\(\rightarrow [Plan]\)</span></li>
<li><strong>Collect the necessary data for exploratory data analysis (EDA)</strong>: This step combines the <span class="math inline">\(Collect\)</span>, <span class="math inline">\(Clean\)</span>, and <span class="math inline">\(Explore\)</span> phases. Involves the iterative “curation” of data need to conduct EDA. <span class="math inline">\(\rightarrow [Collect, Clean, Explore]\)</span></li>
<li><strong>Determine the functional form of the model</strong>: In this step, the specific from of the model is defined, including the definition and characterization of the target variable. This step involves model selection and would in practice be closely associated with the next. <span class="math inline">\(\rightarrow [Prepare, Model]\)</span></li>
<li><strong>Split the data into training and validation</strong> This step is concerned with model validation and avoiding overfitting. Data are divided into training and test datasets. It is assumed that test data were separated out prior to the preceding step. Presumably this step includes fitting the models, but this is not explicit. <span class="math inline">\(\rightarrow [Prepare, Model]\)</span></li>
<li><strong>Assess the model performance</strong>: This step includes determining the stability of a model over time (generalizability), focusing on the overall fit of the model, the significance of each predictor, and the relationship between the target variable and each predictor. Includes measures such as lift. Clearly this step follows the process of fitting and tuning models. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Deploy the model for real-time prediction</strong>: The deployment of machine learning models into production, e.g.&nbsp;via batch prediction as a webservice. <span class="math inline">\(\rightarrow [Deploy]\)</span></li>
<li><strong>Re-build the model</strong>: This step involves revisiting the pipeline as models lose their predictability due to a variety of causes. Effectively, this step asserts the cyclic and interative nature of the pipeline and therefore belongs to no step in particular.</li>
</ol>
</section>
<section id="porter-on-data-science" class="level3">
<h3 class="anchored" data-anchor-id="porter-on-data-science">Porter on Data Science</h3>
<p>Key: <code>Porter</code><br>
Year: 2020<br>
Source: <span class="citation" data-cites="porterFrameworkDataScience2020">@porterFrameworkDataScience2020</span><br>
Bias: Statistics <!--
file:///private/var/folders/14/rnyfspnx2q131jp_752t9fc80000gn/T/com.microsoft.Outlook/Outlook%20Temp/data-science%5B44%5D.html#categories_of_data_science
--></p>
<p>Michael Porter is an Associate Professor of Data Science and Systems Engineering at the UVA. This essay is an internal report (available on request) on the field of data science from the perspective of curricular planning. Porter argues that Data Science includes seven areas, each of which can be viewed as a science, i.e.&nbsp;as requiring specific expertise. Like Donoho’s essay (and the current), the model presented is more abstract than a pipeline model and includes areas that cross-cut steps in the Primary Sequence. Nevertheless, it retains a sequential structure consistent with the general pattern.</p>
<ol type="1">
<li><strong>Data Collection and Acquisition</strong>: The science of “how” and “when” data is collected, and includes all methods of data acquisition from its production through designed experiments to its consumption from external sources, e.g.&nbsp;databases and APIs. <span class="math inline">\(\rightarrow [Collect]\)</span></li>
<li><strong>Data Storage and Representation</strong>: The science of “how” and “when” data is collected, including data modeling and storing data in databases and files of various formats. Also includes transforming data into “tidy” format. <span class="math inline">\(\rightarrow [Store]\)</span></li>
<li><strong>Data Manipulation and Transformation</strong>: The science of preparing data for analysis, including wrangling, cleaning, and importing data from databases (after they have been stored in the previous step). <span class="math inline">\(\rightarrow [Clean, Prepare]\)</span></li>
<li><strong>Computing with Data</strong>: The science of computing for data analysis with a focus on algorithm design and performance evaluation. <span class="math inline">\(\rightarrow [Model]\)</span></li>
<li><strong>Data Analytics</strong>: The science of machine learning, broadly conceived to include methods ranging from geneative modeling (either frequentist or Bayesian) and inference to predictive modeling and optimization. Notably, this step also includes EDA and feature engineering. <span class="math inline">\(\rightarrow [Explore, Prepare, Model]\)</span></li>
<li><strong>Summarizing and Communicating Data and Models</strong>: The science of extracting and summarizing the information in data for human understanding. This area includes visualization in the context of both EDA and presentation of results to external stakeholders. It also includes the communication of model and data properties (such as bias) to guide interpretation of results. Here we map it to the latter, but note that this area includes at least two steps. In addition, we may may the work of summarization to interpretation. <span class="math inline">\(\rightarrow [Interpret, Communicate]\)</span></li>
<li><strong>Practicing Data Science</strong>: The science of the overall system of data science, including improving the data science spipeline, replicability of results, openness and transparency, project management, etc. <span class="math inline">\(\rightarrow [Reflect]\)</span></li>
<li><strong>Disciplinary Data Science</strong>: The science of applying data science to specific disciplines. This involves a consideration of how the pipeline operates in different contexts, including how domain knowledge informs each of the steps of the pipeline, from mode of data acquisition to model selection and analytic appraoch, to the interpretation and communication of results. Although placed at the end of the list, it properly belongs to the initial <span class="math inline">\(Understand\)</span> step. <span class="math inline">\(\rightarrow [Understand]\)</span></li>
</ol>
<!--
## Variations 

**Shape**

* Linear
* Circular

**Granularity**

* Few
* Many -- see CRISP-DM and levels of description.

**Abstraction**

* Pipeline stages only
* Pipeline stages and cross-cutting, abstract principles

## Other Comments

* The place of "exploration"
* Changes in order are bound
* The explicit and the implicit. Sometimes unspecified steps are implied; other times their absence is telling.
-->
<!--
## Comments
-   Place of clustering
-   Place of explorations
-   Cycles and pipes
-   Missing steps, implicit steps
-   Differences in order mostly confined to groups
-   Right even when wrong -- the end points $I$ and $V$, and the middle $III$, remain constant; the arms, as it were, are where it gets messy.
-   Actually, two models -- pipeline and cycle ... Both are sequences, and both stress repetition . . .
-->
</section>
</section>
<section id="summary-table" class="level2">
<h2 class="anchored" data-anchor-id="summary-table">Summary Table</h2>
<p><img src="images/summary-table-screenshot.png" class="img-fluid" alt="Summary Table"></p>

<!--

``` python
import pandas as pd
import re
```



``` python
data = []
cols = ['Source', 'Std Phase', 'Src Phase']
for source in pipes:
    for row in pipes[source].lower().split("\n")[1:-1]:
        data.append([source] + re.split('\s*\|\s*', row))

# Convert CSV to wide table (using Xes)
df = pd.DataFrame(data, columns=cols)\
  .reset_index(drop=True)\
  .set_index(cols[:2])
# df['Src Phase'] = df['Src Phase'].astype('bool').astype('int') 
df.loc[df['Src Phase'] != '', 'Src Phase'] = '■' # '▪︎' '●' 
df = df.unstack()
df.columns = df.columns.droplevel(0)

# Convert CSV to wide table using names as values
# df = pd.DataFrame(data, columns=cols)\
#   .reset_index(drop=True)\
#   .set_index(cols[:2]).unstack() 
# df.columns = df.columns.droplevel(0)

# Preserve sorting
df = df[[phase.lower() for phase in phases]]
df = df.T[sources].T
# df.columns = [col[:3] for col in df.columns]
# df.columns = [str(i+1).zfill(2) for i in range(len(phases))]

df.columns.name = ''
df.index.name = ''
```

``` python

styles = [
  {
    'selector': 'th',
    'props': [
      ('font-weight', 'normal')
    ]
  },
  {
    'selector': "th.col_heading",
    'props': [
      ('width', '40px'),
      ('writing-mode', 'vertical-rl'),
      ('transform', 'rotateZ(210deg)'), 
      ('vertical-align', 'bottom')
    ]
  },
  {
    'selector': 'th.row_heading',
    'props': [
      ('width', '100px'),
      ('text-align', 'right'),
      ('padding-right', '1rem')
    ]
  },
  {
    'selector': 'td.data',
    'props': [
      ('color', 'darkgray')
    ]
  }
]

df.style.set_table_styles(styles)
```
-->
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>